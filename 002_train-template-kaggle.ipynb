{"cells":[{"metadata":{"id":"PR3Ri7_uvmJ1","trusted":true},"cell_type":"code","source":"!pip install pytorch-lightning wandb --upgrade --quiet\n!git clone https://github.com/benihime91/leaf-disease-classification-kaggle.git","execution_count":null,"outputs":[]},{"metadata":{"id":"BlKPGbFqxCk9","trusted":true},"cell_type":"code","source":"import warnings\nimport os\n\nos.chdir(\"/kaggle/working/leaf-disease-classification-kaggle/\")\nwarnings.filterwarnings(\"ignore\")\n\n%matplotlib inline\n%load_ext autoreload\n%autoreload 2","execution_count":null,"outputs":[]},{"metadata":{"id":"hqtky2cXxaV1","trusted":true},"cell_type":"code","source":"# --------------------------------\n# IMPORT LIBRARIES\n# --------------------------------\nimport pytorch_lightning as pl\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torchvision\nimport logging\nimport argparse\nimport pandas as pd\nimport wandb\n\nfrom lightning import LightningModel_resnext50_32x4d as LitModel\nfrom lightning import LitDatatModule\nfrom preprocess import Preprocessor\npd.set_option(\"display.max_colwidth\", None)\n\n# set up paths to the data directories\nimage_dir = \"/kaggle/input/cassava-leaf-disease-classification/train_images\"\ncsv_dir   = \"/kaggle/input/cassava-leaf-disease-classification/train.csv\"\njson_dir  = \"/kaggle/input/cassava-leaf-disease-classification/label_num_to_disease_map.json\"\n\n# set random seeds\nrandom.seed(42)\npl.seed_everything(42)\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# login to wandb: a74f67fd5fae293e301ea8b6710ee0241f595a63\n! wandb login \"a74f67fd5fae293e301ea8b6710ee0241f595a63\"","execution_count":null,"outputs":[]},{"metadata":{"id":"a6v2WKT4ySYy","trusted":true},"cell_type":"code","source":"# since we already have the fold dataset\nfold_csv = \"/kaggle/working/leaf-disease-classification-kaggle/fold_df.csv\"\nprocessor = Preprocessor(csv_dir, json_dir, image_dir, num_folds=5)\n# set the dataframe of Preprocessor to the the fold_csv\ndf = pd.read_csv(fold_csv)\ndf.filePath = [os.path.join(image_dir, df.image_id[i]) for i in range(len(df))]\nprocessor.dataframe = df\nprocessor.dataframe.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"9IpbQMYGycTa","trusted":true},"cell_type":"code","source":"# -------------------------------\n# Grab one FOLD\n# -------------------------------\nfold_num = 0\ntrainFold, valFold = processor.get_fold(fold_num)\ntestFold, valFold  = train_test_split(valFold, stratify=valFold.label, test_size=0.5) \n\ntrainFold.reset_index(drop=True, inplace=True)\ntestFold.reset_index(drop=True, inplace=True)\nvalFold.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"SIhsVOadzTi7","trusted":true},"cell_type":"code","source":"print(\"Length of train data:\", len(trainFold))\nprint(\"Length of test data:\", len(testFold))\nprint(\"Length of valid data:\", len(valFold))","execution_count":null,"outputs":[]},{"metadata":{"id":"ZY9xMCS4zmS5","trusted":true},"cell_type":"code","source":"weights = processor.weights\nweights = torch.tensor(list(weights.values()))\nweights = 1 - weights\nweights","execution_count":null,"outputs":[]},{"metadata":{"id":"IN3bH1iyzzCD","trusted":true},"cell_type":"code","source":"label_map = processor.label_map\nlabel_map","execution_count":null,"outputs":[]},{"metadata":{"id":"ygdIUtF82Yf6","trusted":true},"cell_type":"code","source":"def imshow(image, targets):\n    \"\"\"Imshow for Tensor.\"\"\"\n    grid = torchvision.utils.make_grid(image, normalize=True, nrow=4).permute(1, 2, 0).data.numpy()\n    grid = np.array(grid * 255., dtype=np.uint)\n    classes = targets.data.numpy()\n    plt.figure(figsize=(15, 10))\n    plt.axis(\"off\")\n    plt.imshow(grid)\n    plt.title([label_map[i] for i in classes]);","execution_count":null,"outputs":[]},{"metadata":{"id":"UPAj69ZG0KAp","trusted":true},"cell_type":"code","source":"image_dim = 224 # dimension of the image after resize\n\n# Specify TRANSFORATIONS for TRAIN/VAL/TEST DATALOADERS\ntrain_transformations = A.Compose([\n    A.Rotate(p=0.5, limit=60),\n    A.HorizontalFlip(p=0.5),\n    A.RandomBrightnessContrast(p=0.5),\n    A.CLAHE(p=0.5),\n    A.ShiftScaleRotate(p=0.5),\n    A.Resize(image_dim, image_dim, always_apply=True),\n    A.Normalize(always_apply=True),\n    ToTensorV2(always_apply=True),\n\n])\n\nvalid_transformations = A.Compose([\n    A.Resize(image_dim, image_dim, always_apply=True),\n    A.Normalize(always_apply=True),\n    ToTensorV2(always_apply=True)\n])\n\ntest_transformations = valid_transformations\n\nalbu_transforms = {\n    \"train\": train_transformations, \n    \"valid\": valid_transformations,\n    \"test\" : test_transformations,\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Init DataModule\nbatch_size  = 256\ndata_module = LitDatatModule(trainFold, valFold, testFold, batch_size, albu_transforms, num_workers=3, pin_memory=True)\ndata_module.setup()\n\n# grab samples to log predictions on\nsamples = next(iter(data_module.val_dataloader()))\n\n# view sample images\nims, targs = next(iter(data_module.train_dataloader()))\nimshow(ims[:4], targs[:4])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ----------------------------------\n# TRAINING ARGUMENTS\n# ------------------------------------\nnum_epochs = 20\nsteps_per_epoch = len(data_module.train_dataloader())\ntotal_steps = num_epochs * steps_per_epoch\n\nlearning_rate = 3e-04\nweight_decay = 0.0001\n\noutput_dims = len(label_map)\n\n# Parse arguments\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--learning_rate\", type=float, default=learning_rate, help=\"AdamW: learning rate\")\nparser.add_argument(\"--weight_decay\",  type=float, default=weight_decay, help=\"AdamW: weight_decay\")\nparser.add_argument(\"--total_steps\",   type=int, default=total_steps, help=\"total steps to train for\")\nparser.add_argument(\"--output_dims\",   type=int, default=output_dims, help=\"number of output classes\")\nargs, _ = parser.parse_known_args()\n\nlogger = logging.getLogger(\"lightning\")\nlogger.info(f\"num_epochs: {num_epochs}\")\nlogger.info(f\"steps_per_epoch: {steps_per_epoch}\")\nlogger.info(f\"total_steps: {total_steps}\")\nlogger.info(f\"learning_rate: {learning_rate}\")\nlogger.info(f\"weight_decay: {weight_decay}\")\nlogger.info(f\"output_dims: {output_dims}\")","execution_count":null,"outputs":[]},{"metadata":{"id":"L5aSPEG42gQE","trusted":true},"cell_type":"code","source":"# -----------------------------------\n# LIGHTNING TRAINER\n# ------------------------------------\nclass ImagePredictionLogger(pl.Callback):\n    def __init__(self, val_samples, num_samples = 16):\n        \"\"\"\n        Upon finishing training log num_samples number\n        of images and their predictions to wandb\n        \"\"\"\n        super().__init__()\n        self.val_imgs, self.val_labels = val_samples\n        self.val_imgs   = self.val_imgs[:num_samples]\n        self.val_labels = self.val_labels[:num_samples]\n          \n    def on_fit_end(self, trainer, pl_module):\n        val_imgs = self.val_imgs.to(device=pl_module.device)\n        logits = pl_module(val_imgs)\n        preds = torch.argmax(logits, -1)\n        examples = [wandb.Image(x, caption=f\"Pred:{pred}, Label:{y}\") for x, pred, y in zip(val_imgs, preds, self.val_labels)]\n        trainer.logger.experiment.log({\"examples\": examples})\n\n# Init trainer callbacks\nPATH = \"/kaggle/working/\"\n# os.makedirs(PATH, exist_ok=True)\nchkpt = pl.callbacks.ModelCheckpoint(filepath=PATH, monitor=\"val_loss\", save_top_k=1, mode=\"min\")\n\nlr_monitor = pl.callbacks.LearningRateMonitor(logging_interval=\"step\")\n\nrun_name = f\"resnext50_32x4d|fold={fold_num}\"\nwb_logger = pl.loggers.WandbLogger(project=\"kaggle-leaf-disease\", name=run_name)\nwb_logger.log_hyperparams(vars(args))\n\nstopping = pl.callbacks.EarlyStopping(monitor=\"val_loss\", patience=3, mode=\"min\")\n\ncbs = [lr_monitor, ImagePredictionLogger(samples), stopping]\n\n# Init trainer\ntrainer = pl.Trainer(precision=16,gpus=1,logger=wb_logger,checkpoint_callback=chkpt,callbacks=cbs,max_steps=total_steps,log_every_n_steps=20)","execution_count":null,"outputs":[]},{"metadata":{"id":"WwOuPHvH2zHf","trusted":true},"cell_type":"code","source":"# -------------------------------\n# INSTANTIATE AND FIT MODEL  :\n# --------------------------------\n# Init model\nmodel = LitModel(**vars(args), class_weights=weights, hidden_dims=512)\nmodel.example_input_array = torch.zeros_like(ims)\n\n# Freeze the feature extractor/base of the model\nmodel.freeze_classifier()\n\n# Log model topology \nwb_logger.watch(model.net)\n\n# Pass the datamodule as arg to trainer.fit to override model hooks :)\ntrainer.fit(model, datamodule=data_module)","execution_count":null,"outputs":[]},{"metadata":{"id":"iYFL6YPLDEIc","trusted":true},"cell_type":"code","source":"# Compute metrics on test dataset\ntrainer.test(model, datamodule=data_module)\nwandb.finish()","execution_count":null,"outputs":[]},{"metadata":{"id":"rUejJi686t4p","trusted":true},"cell_type":"code","source":"weightsPath = f\"/kaggle/working/weights_fold={fold_num}.pt\"\n\ntorchmodel = model.net\n# save torch model state dict\ntorch.save(torchmodel.state_dict(), weightsPath)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check save file\nfrom torch import nn\nfrom torchvision import models\n\ndevice = torch.device(\"cuda:0\")\n\n# assemble classification network\nclassifier = models.resnext50_32x4d(pretrained=True, progress=True)\nbase_output_dims = classifier.fc.out_features\nhidden_dims = 512\n\nhidden_layers = nn.Sequential(\n    nn.BatchNorm1d(base_output_dims),\n    nn.Dropout(0.25),\n    nn.ReLU(inplace=True),\n    nn.Linear(base_output_dims, hidden_dims),\n    nn.BatchNorm1d(hidden_dims),\n    nn.Dropout(0.25),\n    nn.ReLU(inplace=True),\n    nn.Linear(hidden_dims, output_dims)\n)\n\nnet = nn.Sequential(classifier, hidden_layers)\n\n# load saved model weights\nstate_dict = torch.load(weightsPath, map_location=device)\n# load the torch state dict to the model\nnet.load_state_dict(state_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}