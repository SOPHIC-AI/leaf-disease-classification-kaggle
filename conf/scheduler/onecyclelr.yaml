scheduler:
  class_name: torch.optim.lr_scheduler.OneCycleLR
  params:
    max_lr: ${training.lr}
    anneal_strategy: cos
    cycle_momentum: true
    epochs: ${training.num_epochs}
    steps_per_epoch: ${training.steps_per_epoch}
  interval: step
  frequency: 1
  monitor: null
