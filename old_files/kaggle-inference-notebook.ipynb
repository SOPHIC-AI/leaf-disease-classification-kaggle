{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from torch import nn\nimport torch\nfrom torchvision import models\nfrom typing import Union\nimport cv2\nfrom tqdm.auto import tqdm\nimport numpy as np\nimport pandas as pd\nimport os\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom inference import InferenceDs, InferenceModel, Predictor\nimport numpy as np","execution_count":6,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"test_df = pd.read_csv(\"../input/cassava-leaf-disease-classification/sample_submission.csv\")\ntest_dir = \"../input/cassava-leaf-disease-classification/test_images\"\ntest_df[\"filePath\"] = [os.path.join(test_dir,test_df[\"image_id\"][n]) for n in range(len(test_df))]\n\n\ntrain_df = pd.read_csv(\"../input/cassava-leaf-disease-classification/train.csv\")\n\nweights = {}\nfor i in range(5):\n    weights[i] = 1 - (len(train_df.loc[train_df.label == i]) / len(train_df))\nweights = list(weights.values())\nweights = np.array(weights)\nweights /= sum(weights)\nprint(weights)","execution_count":7,"outputs":[{"output_type":"stream","text":"[0.23729962 0.22442398 0.22212226 0.09626349 0.21989064]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# path to saved weights file\nWEIGHTS_PATH = \"../input/leafdiseaseclassificationmodelweights/weights_fold0.pt\"\n\nimage_list = list(os.listdir('../input/cassava-leaf-disease-classification/test_images/'))\n\n# model feature extractor\nclassifier = models.resnext50_32x4d()\n\n# dims for the base model\nnum_ftrs = classifier.fc.out_features\nh1 = 512 \nh2 = int(h1/2)\n\n# base of the feature extractor\nbase_model = nn.Sequential(\n    nn.BatchNorm1d(num_ftrs),\n    nn.ReLU(inplace=True),\n    nn.Dropout(0.25),\n    nn.Linear(num_ftrs, h1),\n    nn.BatchNorm1d(h1),\n    nn.ReLU(inplace=True),\n    nn.Dropout(0.25),\n    nn.Linear(h1, h2),\n    nn.BatchNorm1d(h2),\n    nn.ReLU(inplace=True),\n    nn.Dropout(0.5),\n    nn.Linear(h2, 5)\n)\n\n# test time augmentations\ntest_augs = A.Compose([\n    A.RandomResizedCrop(224, 224),\n    A.Transpose(p=0.5),\n    A.HorizontalFlip(p=0.5),\n    A.VerticalFlip(p=0.5),\n    A.HueSaturationValue(p=0.5),\n    A.RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n    A.Normalize(max_pixel_value=255.0, p=1.0),\n    ToTensorV2(p=1.0),\n],p=1.0)\n\n\ndef inference_one_epoch(model, data_loader, device):\n    \"\"\"\n    Performs one evaluations step\n    \"\"\"\n    model.eval()\n    \n    image_preds_all = []\n    \n    pbar = tqdm(enumerate(data_loader), total=len(data_loader))\n    \n    for step, (imgs, ids) in pbar:\n        imgs = imgs.to(device)\n        image_preds = model(imgs)\n        image_preds_all += [torch.softmax(image_preds, 1).detach().cpu().numpy()] \n    \n    image_preds_all = np.concatenate(image_preds_all, axis=0)\n    \n    return image_preds_all","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if __name__ == '__main__':\n    tta_folds = 5\n    # device\n    device  = torch.device(\"cuda:0\") if torch.cuda.is_available() else \"cpu\"\n    \n    # test dataframe\n    test = pd.DataFrame()\n    test[\"image_id\"] = image_list\n    test[\"filePath\"] = [os.path.join(test_dir, test[\"image_id\"][n]) for n in range(len(test))]\n    \n    # dataloader(s)\n    tst_loader = torch.utils.data.DataLoader(InferenceDs(test, test_augs), shuffle=False, batch_size=128)\n    \n    # init model\n    model = InferenceModel(classifier=classifier, base=base_model)\n    model.load_state_dict(torch.load(WEIGHTS_PATH))\n    model.to(device)\n        \n    # store predictions\n    tst_preds = []\n    \n    \n    # generate predictions\n    with torch.no_grad():\n        for _ in tqdm(range(tta_folds), desc=\"Eval\"):\n            tst_preds += [(weights / tta_folds)*inference_one_epoch(model, tst_loader, device)]\n        \n        tst_preds = np.mean(tst_preds, axis=0) \n\n    del model\n    torch.cuda.empty_cache()","execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=5.0, style=ProgressStyle(description_widâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a53aae5522ae46e39970c3157f56d3f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5555fb4edfb54b45b01937a41c2d3976"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"761a7e08982e45dfabc023ce5350cf56"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"614faabd92624cf58609c863df72c822"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"916bb48d9d22461698a8cf0d8c979b2d"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af86fb1b00c64241b781f4004e0f8c5b"}},"metadata":{}},{"output_type":"stream","text":"\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test[\"label\"] = np.argmax(tst_preds, axis=1)\ntest.drop(columns=[\"filePath\"], inplace=True)\ntest.head()","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"         image_id  label\n0  2216849948.jpg      2","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2216849948.jpg</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}